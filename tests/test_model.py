
def test_yourtts_download_model():
    # Load model directly
    from transformers import AutoModel
    cache_dir = "models/tts/yourtts/base"
    model = AutoModel.from_pretrained("wannaphong/khanomtan-tts-v1.0",cache_dir=cache_dir)

def test_f5_tts():
    import torch
    import numpy as np
    import soundfile as sf
    from omegaconf import OmegaConf

    from f5_tts.infer.utils_infer import (
        preprocess_ref_audio_text,
        load_model,
        load_vocoder,
        get_tokenizer,
        infer_process,
        target_sample_rate,
        n_fft,
        hop_length,
        win_length,
        n_mel_channels,
    )
    from f5_tts.model import DiT
    from f5_tts.model.utils import seed_everything

    # ========== è·¯å¾„é…ç½® ==========
    MODEL_PATH = r"/models/tts/f5_tts/thai/model.pt"
    VOCAB_PATH = r"D:\github\voice-translate-pro\models\tts\f5_tts\thai\vocab.txt"
    REF_AUDIO_PATH = r"D:\github\voice-translate-pro\tests\ref.wav"
    REF_TEXT = "à¹„à¸”à¹‰à¸£à¸±à¸šà¸‚à¹ˆà¸²à¸§à¸„à¸£à¸²à¸§à¸‚à¸­à¸‡à¹€à¸£à¸²à¸—à¸µà¹ˆà¸ˆà¸°à¸«à¸²à¸—à¸µà¹ˆà¸¡à¸±à¸™à¹€à¸›à¹‡à¸™à¹„à¸›à¸—à¸µà¹ˆà¸ˆà¸°à¸ˆà¸±à¸”à¸‚à¸¶à¹‰à¸™."  # æ›¿æ¢ä¸ºä½ çš„å‚è€ƒè¯­éŸ³çš„å†…å®¹
    GEN_TEXT = "à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¹ˆà¸° à¸¢à¸´à¸™à¸”à¸µà¸•à¹‰à¸­à¸™à¸£à¸±à¸šà¹€à¸‚à¹‰à¸²à¸ªà¸¹à¹ˆà¸£à¹‰à¸²à¸™à¸‚à¸­à¸‡à¹€à¸£à¸² à¸«à¸§à¸±à¸‡à¸§à¹ˆà¸²à¸„à¸¸à¸“à¸ˆà¸°à¸¡à¸µà¸„à¸§à¸²à¸¡à¸ªà¸¸à¸‚à¸à¸±à¸šà¸à¸²à¸£à¹€à¸¥à¸·à¸­à¸à¸‹à¸·à¹‰à¸­à¸ªà¸´à¸™à¸„à¹‰à¸²à¹ƒà¸™à¸§à¸±à¸™à¸™à¸µà¹‰"
    OUTPUT_PATH = f"""output_{2}.wav"""

    device = "cuda" if torch.cuda.is_available() else "cpu"
    mel_spec_type = "vocos"
    tokenizer = "custom"

    # ========== åŠ è½½ tokenizer ==========
    vocab_char_map, vocab_size = get_tokenizer(VOCAB_PATH, tokenizer)

    # ========== æ¨¡å‹å‚æ•°é…ç½® ==========
    model_cfg = dict(
        dim=1024,
        depth=22,
        heads=16,
        ff_mult=2,
        text_dim=512,
        conv_layers=4
    )

    # ========== æ„å»ºæ¨¡å‹ï¼ˆè‡ªåŠ¨å°è£…ä¸º CFMï¼‰==========
    print("ğŸ› æ­£åœ¨æ„å»ºæ¨¡å‹...")
    model = load_model(
        model_cls=DiT,
        model_cfg=model_cfg,
        ckpt_path=MODEL_PATH,
        mel_spec_type=mel_spec_type,
        vocab_file=VOCAB_PATH,
        use_ema=True,
        device=device
    )
    model.eval()

    # ========== åŠ è½½ vocoder ==========
    vocoder = load_vocoder(vocoder_name=mel_spec_type, is_local=False, device=device)

    # ========== åŠ è½½å‚è€ƒéŸ³é¢‘ ==========
    ref_audio_tensor, ref_text_proc = preprocess_ref_audio_text(REF_AUDIO_PATH, REF_TEXT)

    # ========== åˆæˆè¯­éŸ³ ==========
    print("ğŸ™ å¼€å§‹åˆæˆè¯­éŸ³...")
    seed_everything(42)

    # | å‚æ•°å                  | å½“å‰å€¼  | å»ºè®®è°ƒæ•´èŒƒå›´     | ä½œç”¨                        |
    # | -------------------- | ---- | ---------- | ------------------------- |
    # | `cfg_strength`       | 2.0  | 1.5 â€“ 2.8  | æ§åˆ¶ç”ŸæˆéŸ³é¢‘ä¸å‚è€ƒé£æ ¼çš„ç›¸ä¼¼åº¦ï¼ˆè¶Šé«˜è¶Šæ¥è¿‘å‚è€ƒéŸ³ï¼‰ |
    # | `sway_sampling_coef` | -1.0 | -2.0 â€“ 0.0 | æ§åˆ¶æƒ…æ„Ÿé£æ ¼çš„â€œæ‘‡æ‘†â€å¹…åº¦ï¼Œæå‡è¯­æ°”å˜åŒ–æ„Ÿ     |
    # | `speed`              | 1.0  | 0.95 â€“ 1.2 | å¾®è°ƒè¯­é€Ÿï¼Œç¨å¿«è¯­é€Ÿåœ¨ç›´æ’­å¸¦è´§ä¸­æ›´æœ‰â€œå¸å¼•åŠ›â€    |
    # | `nfe_step`           | 32   | 40 â€“ 48    | å¢åŠ å¯ä»¥æ”¹å–„åˆæˆè´¨é‡ï¼ˆä½†å¯èƒ½å¸¦æ¥å»¶è¿Ÿï¼‰       |


    speed = 1.05
    cfg_strength = 2.8
    cross_fade_duration = 0.12
    nfe_step = 48
    sway_sampling_coef = 0.2
    target_rms = -18.0
    fix_duration = None


    audio, sample_rate, _ = infer_process(
            ref_audio=ref_audio_tensor,
            ref_text=ref_text_proc,
            gen_text=GEN_TEXT,
            model_obj=model,
            vocoder=vocoder,
            mel_spec_type=mel_spec_type,
            target_rms=target_rms,
            cross_fade_duration=cross_fade_duration,
            nfe_step=nfe_step,
            cfg_strength=cfg_strength,
            sway_sampling_coef=sway_sampling_coef,
            speed=speed,
            fix_duration=fix_duration,
            device=torch.device(device)
        )

    # ========== ä¿å­˜ç»“æœ ==========
    sf.write(OUTPUT_PATH, audio, sample_rate)
    print(f"âœ… åˆæˆå®Œæˆï¼Œä¿å­˜ä¸ºï¼š{OUTPUT_PATH}")

def test_live_f5_tts():
    import torch
    import soundfile as sf
    from omegaconf import OmegaConf

    from f5_tts.infer.utils_infer import (
        preprocess_ref_audio_text,
        load_model,
        load_vocoder,
        infer_process,
        get_tokenizer,
        n_fft, hop_length, win_length,
        n_mel_channels, target_sample_rate,
    )

    from f5_tts.model import UNetT
    from f5_tts.model.utils import seed_everything


    def live_tts(
            ref_audio_path: str,
            ref_text: str,
            gen_text: str,
            model_path: str = "model.pt",
            vocab_path: str = "vocab.txt",
            output_path: str = "live_output.wav",
            use_cuda: bool = True
    ):
        # ===== è®¾ç½®è®¾å¤‡ =====
        device = "cuda" if use_cuda and torch.cuda.is_available() else "cpu"
        device_torch = torch.device(device)

        # ===== åŠ è½½ tokenizer =====
        vocab_char_map, vocab_size = get_tokenizer(vocab_path, "custom")

        # ===== æ¨¡å‹é…ç½®ï¼ˆUNetT æ›´é€‚åˆæƒ…æ„Ÿè¯­è°ƒï¼‰ =====
        model_cfg = dict(
            dim=1024,
            depth=20,
            heads=16,
            ff_mult=2,
            text_dim=512,
            conv_layers=4
        )

        # ===== åŠ è½½æ¨¡å‹ï¼ˆå°è£…ä¸º CFMï¼‰ =====
        model = load_model(
            model_cls=UNetT,
            model_cfg=model_cfg,
            ckpt_path=model_path,
            mel_spec_type="bigvgan",
            vocab_file=vocab_path,
            use_ema=True,
            device=device
        )
        model.eval()

        # ===== åŠ è½½ vocoder =====
        vocoder = load_vocoder(vocoder_name="bigvgan", is_local=False, device=device)

        # ===== é¢„å¤„ç†å‚è€ƒè¯­éŸ³ =====
        ref_audio_tensor, ref_text_proc = preprocess_ref_audio_text(ref_audio_path, ref_text)

        # ===== è®¾ç½®éšæœºç§å­ç¡®ä¿ä¸€è‡´æ€§ =====
        seed_everything(42)

        # ===== åˆæˆè¯­éŸ³ =====
        audio, sample_rate, _ = infer_process(
            ref_audio=ref_audio_tensor,
            ref_text=ref_text_proc,
            gen_text=gen_text,
            model_obj=model,
            vocoder=vocoder,
            mel_spec_type="bigvgan",
            target_rms=-19.0,
            cross_fade_duration=0.12,
            nfe_step=48,
            cfg_strength=2.8,
            sway_sampling_coef=-1.0,
            speed=1.05,
            fix_duration=None,
            device=device_torch,
        )

        # ===== ä¿å­˜è¾“å‡ºéŸ³é¢‘ =====
        sf.write(output_path, audio, sample_rate)
        print(f"âœ… åˆæˆå®Œæˆï¼Œè¾“å‡ºè·¯å¾„: {output_path}")

    MODEL_PATH = r"/models/tts/f5_tts/thai/model.pt"
    VOCAB_PATH = r"D:\github\voice-translate-pro\models\tts\f5_tts\thai\vocab.txt"
    REF_AUDIO_PATH = r"D:\github\voice-translate-pro\tests\ref.wav"
    REF_TEXT = "à¹„à¸”à¹‰à¸£à¸±à¸šà¸‚à¹ˆà¸²à¸§à¸„à¸£à¸²à¸§à¸‚à¸­à¸‡à¹€à¸£à¸²à¸—à¸µà¹ˆà¸ˆà¸°à¸«à¸²à¸—à¸µà¹ˆà¸¡à¸±à¸™à¹€à¸›à¹‡à¸™à¹„à¸›à¸—à¸µà¹ˆà¸ˆà¸°à¸ˆà¸±à¸”à¸‚à¸¶à¹‰à¸™."  # æ›¿æ¢ä¸ºä½ çš„å‚è€ƒè¯­éŸ³çš„å†…å®¹
    GEN_TEXT = "à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¹ˆà¸° à¸¢à¸´à¸™à¸”à¸µà¸•à¹‰à¸­à¸™à¸£à¸±à¸šà¹€à¸‚à¹‰à¸²à¸ªà¸¹à¹ˆà¸£à¹‰à¸²à¸™à¸‚à¸­à¸‡à¹€à¸£à¸² à¸«à¸§à¸±à¸‡à¸§à¹ˆà¸²à¸„à¸¸à¸“à¸ˆà¸°à¸¡à¸µà¸„à¸§à¸²à¸¡à¸ªà¸¸à¸‚à¸à¸±à¸šà¸à¸²à¸£à¹€à¸¥à¸·à¸­à¸à¸‹à¸·à¹‰à¸­à¸ªà¸´à¸™à¸„à¹‰à¸²à¹ƒà¸™à¸§à¸±à¸™à¸™à¸µà¹‰"
    OUTPUT_PATH = f"""live_output_{2}.wav"""

    live_tts(
        ref_audio_path=REF_AUDIO_PATH,
        ref_text=REF_TEXT,
        gen_text=GEN_TEXT,
        model_path=MODEL_PATH,
        vocab_path=VOCAB_PATH,
        output_path=OUTPUT_PATH,
        use_cuda=True,
    )


